{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweety_train = fetch_20newsgroups(subset='train',shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tweety_train.target_names   - gives all category\n",
    "len(tweety_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweety_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"From: lerxst@wam.umd.edu (where's my thing)\",\n",
       " 'Subject: WHAT car is this!?',\n",
       " 'Nntp-Posting-Host: rac3.wam.umd.edu',\n",
       " 'Organization: University of Maryland, College Park',\n",
       " 'Lines: 15',\n",
       " '',\n",
       " ' I was wondering if anyone out there could enlighten me on this car I saw',\n",
       " 'the other day. It was a 2-door sports car, looked to be from the late 60s/',\n",
       " 'early 70s. It was called a Bricklin. The doors were really small. In addition,',\n",
       " 'the front bumper was separate from the rest of the body. This is ',\n",
       " 'all I know. If anyone can tellme a model name, engine specs, years',\n",
       " 'of production, where this car is made, history, or whatever info you',\n",
       " 'have on this funky looking car, please e-mail.',\n",
       " '',\n",
       " 'Thanks,',\n",
       " '- IL',\n",
       " '   ---- brought to you by your neighborhood Lerxst ----',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweety_train['data'][0].split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CountVectorizer\n",
    "Convert a collection of text documents to a matrix of token counts\n",
    "\n",
    "This implementation produces a sparse representation of the counts using\n",
    "scipy.sparse.csr_matrix.\n",
    "\n",
    "If you do not provide an a-priori dictionary and you do not use an analyzer\n",
    "that does some kind of feature selection then the number of features will\n",
    "be equal to the vocabulary size found by analyzing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(tweety_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 130107)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<11314x130107 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1787565 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_train_counts.shape)    #11314 sentences are there and 130107 is features_names\n",
    "X_train_counts  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x130107 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 89 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_counts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X_train_counts.toarray(),columns=count_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second step = TfidfTransformer\n",
    "Transform a count matrix to a normalized tf or tf-idf representation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Transform a count matrix to a normalized tf or tf-idf representation\n",
    "\n",
    "Tf means term-frequency while tf-idf means term-frequency times inverse\n",
    "document-frequency. This is a common term weighting scheme in information\n",
    "retrieval, that has also found good use in document classification.\n",
    "\n",
    "The goal of using tf-idf instead of the raw frequencies of occurrence of a\n",
    "token in a given document is to scale down the impact of tokens that occur\n",
    "very frequently in a given corpus and that are hence empirically less\n",
    "informative than features that occur in a small fraction of the training\n",
    "corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfid_transformer = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfid = tfid_transformer.fit_transform(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 130107)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "  (0, 56979)\t0.0574701540749\n",
      "  (0, 75358)\t0.353835013497\n",
      "  (0, 123162)\t0.259709024574\n",
      "  (0, 118280)\t0.211868072083\n",
      "  (0, 50527)\t0.0546142865886\n",
      "  (0, 124031)\t0.107987951542\n",
      "  (0, 85354)\t0.0369697850882\n",
      "  (0, 114688)\t0.0621407098631\n",
      "  (0, 111322)\t0.019156718025\n",
      "  (0, 123984)\t0.0368542926346\n",
      "  (0, 37780)\t0.381338912595\n",
      "  (0, 68532)\t0.0732581234213\n",
      "  (0, 114731)\t0.144472755128\n",
      "  (0, 87620)\t0.0356718631408\n",
      "  (0, 95162)\t0.0344713840933\n",
      "  (0, 64095)\t0.0354209242713\n",
      "  (0, 98949)\t0.160686060554\n",
      "  (0, 90379)\t0.0199288599566\n",
      "  (0, 118983)\t0.0370859780506\n",
      "  (0, 89362)\t0.065211743063\n",
      "  (0, 79666)\t0.109364012524\n",
      "  (0, 40998)\t0.0780136819692\n",
      "  (0, 92081)\t0.0991327449391\n",
      "  (0, 76032)\t0.0192194630522\n",
      "  (0, 4605)\t0.0633260395248\n",
      "  :\t:\n",
      "  (0, 37565)\t0.0343176044248\n",
      "  (0, 113986)\t0.176917506749\n",
      "  (0, 83256)\t0.0884438249646\n",
      "  (0, 86001)\t0.0700041144584\n",
      "  (0, 51730)\t0.0971474405798\n",
      "  (0, 109271)\t0.108447248221\n",
      "  (0, 128026)\t0.0606220958898\n",
      "  (0, 96144)\t0.108269044907\n",
      "  (0, 78784)\t0.0633940918806\n",
      "  (0, 63363)\t0.0834274838797\n",
      "  (0, 90252)\t0.0318893687954\n",
      "  (0, 123989)\t0.0820702746533\n",
      "  (0, 67156)\t0.0731344392274\n",
      "  (0, 128402)\t0.0592229408328\n",
      "  (0, 62221)\t0.0292152799243\n",
      "  (0, 57308)\t0.155871700916\n",
      "  (0, 76722)\t0.0690877999962\n",
      "  (0, 94362)\t0.0554570313901\n",
      "  (0, 78955)\t0.0598985688806\n",
      "  (0, 114428)\t0.055111051547\n",
      "  (0, 66098)\t0.0978551570831\n",
      "  (0, 35187)\t0.0935393059832\n",
      "  (0, 35983)\t0.0377044856362\n",
      "  (0, 128420)\t0.0427849907928\n",
      "  (0, 86580)\t0.131571187142\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train_tfid))\n",
    "print(X_train_tfid[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = MultinomialNB().fit(X_train_tfid,tweety_train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Same above thing using pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words='english')),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', MultinomialNB()),\n",
    "])\n",
    "text_clf = text_clf.fit(tweety_train.data, tweety_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81691449814126393"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True)\n",
    "predicted = text_clf.predict(twenty_test.data)\n",
    "np.mean(predicted == twenty_test.target)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Initial accuracy is - 77% and after removing stopwords it reaches to 81%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming: \n",
    "From Wikipedia, stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form. E.g. A stemming algorithm reduces the words “fishing”, “fished”, and “fisher” to the root word, “fish”.\n",
    "We need NLTK which can be installed from here. NLTK comes with various stemmers (details on how stemmers work are out of scope for this article) which can help reducing the words to their root form. Again use this, if it make sense for your problem.\n",
    "Below I have used Snowball stemmer which works very well for English language. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])\n",
    "stemmed_count_vect = StemmedCountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81678173127987252"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_mnb_stemmed = Pipeline([('vect', stemmed_count_vect),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('mnb', MultinomialNB(fit_prior=False)),\n",
    " ])\n",
    "text_mnb_stemmed = text_mnb_stemmed.fit(tweety_train.data, tweety_train.target)\n",
    "predicted_mnb_stemmed = text_mnb_stemmed.predict(twenty_test.data)\n",
    "np.mean(predicted_mnb_stemmed == twenty_test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference \n",
    "- http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.get_feature_names\n",
    "- http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer\n",
    "- https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a\n",
    " \n",
    "Topic Modeling \n",
    "- https://medium.com/mlreview/topic-modeling-with-scikit-learn-e80d33668730\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
